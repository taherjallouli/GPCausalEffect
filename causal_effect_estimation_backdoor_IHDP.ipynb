{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing IDHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dowhy import CausalModel\n",
    "\n",
    "path_train = r\"C:\\Users\\User\\OneDrive\\Desktop\\bachelor_thesis\\ihdp_npci_1-1000.train.npz\"\n",
    "path_test = r\"C:\\Users\\User\\OneDrive\\Desktop\\bachelor_thesis\\ihdp_npci_1-1000.test.npz\"\n",
    "#path_train = r\"C:\\Users\\User\\OneDrive\\Desktop\\bachelor_thesis\\ihdp_npci_1-100.train.npz\"\n",
    "#path_test = r\"C:\\Users\\User\\OneDrive\\Desktop\\bachelor_thesis\\ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "\n",
    "\n",
    "data_in_train = np.load(path_train)\n",
    "data_in_test = np.load(path_test)\n",
    "x = np.concatenate((data_in_train['x'], data_in_test['x']))\n",
    "y = np.concatenate((data_in_train['yf'], data_in_test['yf']))\n",
    "t = np.concatenate((data_in_train['t'], data_in_test['t']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IHDP to multiple CSVs for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(y[0])):\n",
    "#    x_i = x[:,:,i]\n",
    "#    y_i = y[:,i]\n",
    "#    t_i = t[:,i]\n",
    "#    col = ['t', 'y']\n",
    "#    for j in range(1,26):\n",
    "#        col.append(\"x\"+str(j))\n",
    "#    df_temp = pd.DataFrame(np.column_stack((t_i, y_i, x_i)), columns= col)\n",
    "#    df_temp.to_csv(f\"IHDP_data_CSVs/ihdp_{i+1}.csv\", index= False)\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, DotProduct\n",
    "def average_std(std_arr):\n",
    "    return 1 / len(std_arr) * np.sqrt(np.square(std_arr).sum()) \n",
    "\n",
    "def gp_ate_backdoor_binary_t(df_no_treatment, df_treatment, df_temp):\n",
    "    kernel = 1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
    "        + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "    gp_0= GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=7,  alpha=0.0)\n",
    "    gp_1= GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=7,  alpha=0.0)\n",
    "\n",
    "    # Fit the GP to the data\n",
    "    gp_0.fit(df_no_treatment[[\"x\"+str(i) for  i in range(1,26)]], df_no_treatment['y'])\n",
    "    gp_1.fit(df_treatment [[\"x\"+str(i) for  i in range(1,26)]], df_treatment['y'])\n",
    "\n",
    "    gp_mu_1, gp_std_1 =  gp_1.predict(df_temp[[\"x\"+str(i) for  i in range(1,26)]],return_std= True) \n",
    "    gp_mu_0, gp_std_0 =  gp_0.predict(df_temp[[\"x\"+str(i) for  i in range(1,26)]],return_std= True)\n",
    "\n",
    "    gp_ate = np.mean(gp_mu_1 - gp_mu_0)\n",
    "    gp_ate_sd = average_std(gp_std_1) + average_std(gp_std_0) \n",
    "    return gp_ate, gp_ate_sd\n",
    "\n",
    "\n",
    "def gp_linear_backdoor_binary_t(df_no_treatment, df_treatment, df_temp):\n",
    "    kernel = DotProduct() + WhiteKernel()\n",
    "    gp_0= GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5,  alpha=0.0)\n",
    "    gp_1= GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5,  alpha=0.0)\n",
    "\n",
    "    # Fit the GP to the data\n",
    "    gp_0.fit(df_no_treatment[[\"x\"+str(i) for  i in range(1,26)]], df_no_treatment['y'])\n",
    "    gp_1.fit(df_treatment [[\"x\"+str(i) for  i in range(1,26)]], df_treatment['y'])\n",
    "\n",
    "    gp_mu_1, gp_std_1 =  gp_1.predict(df_temp[[\"x\"+str(i) for  i in range(1,26)]],return_std= True) \n",
    "    gp_mu_0, gp_std_0 =  gp_0.predict(df_temp[[\"x\"+str(i) for  i in range(1,26)]],return_std= True)\n",
    "\n",
    "    gp_ate = np.mean(gp_mu_1 - gp_mu_0)\n",
    "    gp_ate_sd = average_std(gp_std_1) + average_std(gp_std_0) \n",
    "    return gp_ate, gp_ate_sd\n",
    "\n",
    "def gp_linear_backdoor_binary_t_v2(df_temp):\n",
    "    kernel = DotProduct() + WhiteKernel()\n",
    "    gp= GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5,  alpha=0.0)\n",
    "    \n",
    "\n",
    "    # Fit the GP to the data\n",
    "    col = ['t']+ [\"x\"+str(i) for  i in range(1,26)]\n",
    "    gp.fit(df_temp[col ], df_temp['y'])\n",
    "    x1 = np.array([1] + [0 for  i in range(1,26)])\n",
    "    x0 = np.array([0 for  i in range(1,27)])\n",
    "    gp_lin_ate =  gp.predict(x1.reshape(1,-1)) - gp.predict(x0.reshape(1,-1)) \n",
    "    \n",
    "    return gp_lin_ate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def ipw_ate_backdoor_binary_t(df_temp):\n",
    "    # Create a causal model from the data and given common causes.\n",
    "    model=CausalModel(\n",
    "            data = df_temp,\n",
    "            treatment='t',\n",
    "            outcome='y',\n",
    "            common_causes=[\"x\"+str(i) for  i in range(1,26)]\n",
    "            )\n",
    "\n",
    "    identified_estimand = model.identify_effect(proceed_when_unidentifiable=True, method_name=\"maximal-adjustment\")\n",
    "    # Using Propensity Score Weighting\n",
    "    estimate = model.estimate_effect(identified_estimand,\n",
    "            method_name=\"backdoor.propensity_score_weighting\"\n",
    "    )\n",
    "    return estimate.value\n",
    "\n",
    "\n",
    "def ols_backdoor_binary_t(df_temp):\n",
    "    # Create a causal model from the data and given common causes.\n",
    "    model=CausalModel(\n",
    "            data = df_temp,\n",
    "            treatment='t',\n",
    "            outcome='y',\n",
    "            common_causes=[\"x\"+str(i) for  i in range(1,26)]\n",
    "            )\n",
    "\n",
    "    identified_estimand = model.identify_effect(proceed_when_unidentifiable=True, method_name=\"maximal-adjustment\")\n",
    "\n",
    "    estimate = model.estimate_effect(identified_estimand,\n",
    "            method_name=\"backdoor.linear_regression\"\n",
    "    )\n",
    "    return estimate.value\n",
    "\n",
    "def gp_linear_backdoor_binary_t_pymc(df_temp):\n",
    "    import bambi as bmb\n",
    "    import pymc as pm\n",
    "    \"\"\"\n",
    "    Calculates the Markov Chain Monte Carlo trace of\n",
    "    a Generalised Linear Model Bayesian linear regression \n",
    "    model on supplied data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: `pd.DataFrame`\n",
    "        DataFrame containing the data\n",
    "    iterations: `int`\n",
    "        Number of iterations to carry out MCMC for\n",
    "    \"\"\"\n",
    "    iterations=2500\n",
    "    # Create the glm using the Bambi model syntax   \n",
    "    bmb_string = \"y ~ t + x1\"\n",
    "    for i in range(2,26):\n",
    "        bmb_string += \"+ x\"+str(i)\n",
    "    model = bmb.Model(bmb_string, df_temp)\n",
    "    # Fit the model using a NUTS (No-U-Turn Sampler) \n",
    "    trace = model.fit(\n",
    "        draws= iterations,\n",
    "        tune=400,\n",
    "        discard_tuned_samples=True,\n",
    "        chains=1, \n",
    "        progressbar=True)\n",
    "    return trace.posterior.t.to_numpy().mean()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.dml import CausalForestDML\n",
    "\n",
    "def cf_dml_backdoor_binary(df_temp):\n",
    "    est = CausalForestDML( random_state=123)\n",
    "    est.fit(Y = df_temp.y, T = df_temp.t, X = np.array(df_temp[[\"x\"+str(i) for  i in range(1,26)]]), W =df_temp[[\"x\"+str(i) for  i in range(1,26)]] )\n",
    "\n",
    "    return est.ate(X = np.array(df_temp[[\"x\"+str(i) for  i in range(1,26)]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ate_list= []\n",
    "ipw_ate_list = []\n",
    "ols_ate_list = []\n",
    "tmle_ate_list = []\n",
    "cf_dml_ate_list = []\n",
    "gp_linear_ate_list = []\n",
    "\n",
    "\n",
    "def get_ith_df(i, x, y, t):\n",
    "    x_i = x[:,:,i]\n",
    "    y_i = y[:,i]\n",
    "    t_i = t[:,i]\n",
    "    col = ['t', 'y']\n",
    "    for j in range(1,26):\n",
    "        col.append(\"x\"+str(j))\n",
    "    df_temp = pd.DataFrame(np.column_stack((t_i, y_i, x_i)), columns= col)\n",
    "    return df_temp\n",
    "\n",
    "for i in range(len(y[0])):\n",
    "    \n",
    "    df_temp = get_ith_df(i,  x, y, t)\n",
    "    df_no_treatment = df_temp[df_temp.t == 0]\n",
    "    df_treatment = df_temp[df_temp.t == 1]\n",
    "    print(i+1,\"/\", len(y[0]))\n",
    "    #GP RBF\n",
    "    #gp_ate, gp_ate_sd = gp_ate_backdoor_binary_t(df_no_treatment, df_treatment, df_temp)\n",
    "    #gp_ate_list.append(gp_ate)\n",
    "    \n",
    "    ### Causal Forest DML\n",
    "    #cf_dml_ate_list.append(cf_dml_backdoor_binary(df_temp))\n",
    "\n",
    "    ### IPW\n",
    "    #ipw_ate_list.append(ipw_ate_backdoor_binary_t(df_temp))\n",
    "#\n",
    "    ### OLS\n",
    "    #ols_ate_list.append(ols_backdoor_binary_t(df_temp))\n",
    "\n",
    "    # GP linear\n",
    "    gp_linear_ate_list.append(gp_linear_backdoor_binary_t_pymc(df_temp))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard error of the mean: (ate +- sem(ate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for GP Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5538875379448825 +- 0.0591050715524681\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "sem_gp_ate = scipy.stats.sem(np.array(gp_ate_list) - np.full((len(gp_ate_list),),4))\n",
    "\n",
    "mean_error_gp_rbf =np.mean(np.array(gp_ate_list) - np.full((len(gp_ate_list),),4)) \n",
    "print(mean_error_gp_rbf, \"+-\", sem_gp_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for Causal forest DML (n = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05097218389007281 +- 0.024999134444642947\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "sem_cf_dml_ate = scipy.stats.sem(np.array(cf_dml_ate_list) - np.full((len(cf_dml_ate_list),),4))\n",
    "\n",
    "mean_error_cf_dml =np.abs(np.mean(np.array(cf_dml_ate_list) - np.full((len(cf_dml_ate_list),),4))) \n",
    "print(mean_error_cf_dml, \"+-\", sem_cf_dml_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results for IPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36501475076706896 +- 0.04109369390342119\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "sem_ipw_ate = scipy.stats.sem(np.array(ipw_ate_list) - np.full((len(ipw_ate_list),),4))\n",
    "\n",
    "mean_error_ipw_ate =np.abs(np.mean(np.array(ipw_ate_list) - np.full((len(ipw_ate_list),),4))) \n",
    "print(mean_error_ipw_ate, \"+-\", sem_ipw_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13326951863708864 +- 0.01411529524683612\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "sem_ols_ate = scipy.stats.sem(np.array(ols_ate_list) - np.full((len(ols_ate_list),),4))\n",
    "\n",
    "mean_error_ols_ate =np.abs(np.mean(np.array(ols_ate_list) - np.full((len(ols_ate_list),),4))) \n",
    "print(mean_error_ols_ate, \"+-\", sem_ols_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results for GP linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4940570461222967 +- 0.02317183858855958\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "gp_ate_array_without_std = np.array(gp_linear_ate_list).reshape(1000)\n",
    "sem_gp_linear_ate = scipy.stats.sem(np.array(gp_ate_array_without_std) - np.full((len(gp_ate_array_without_std),),4))\n",
    "\n",
    "mean_error_gp_linear_ate =np.abs(np.mean(np.array(gp_ate_array_without_std) - np.full((len(gp_ate_array_without_std),),4))) \n",
    "print(mean_error_gp_linear_ate, \"+-\", sem_gp_linear_ate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
